{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning albert-base-v2 with LoRA\n",
        "\n",
        "Upload a CSV named `dataset.csv` with columns `text,label` and set `DATASET_PATH` accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# If on Colab, uncomment:\n",
        "# !pip install -U transformers datasets peft accelerate evaluate scikit-learn matplotlib --quiet\n",
        "\n",
        "import os, numpy as np, pandas as pd, evaluate, matplotlib.pyplot as plt\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "DATASET_PATH = \"/content/dataset.csv\"  # CSV with columns: text,label\n",
        "NUM_LABELS = None  # set automatically from data\n",
        "MODEL_NAME = \"albert-base-v2\"\n",
        "OUTPUT_DIR = \"outputs_albert_lora\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "assert \"text\" in df.columns and \"label\" in df.columns\n",
        "labels = sorted(df[\"label\"].astype(str).unique().tolist())\n",
        "label2id = {l:i for i,l in enumerate(labels)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "NUM_LABELS = len(labels)\n",
        "df[\"label_id\"] = df[\"label\"].astype(str).map(label2id)\n",
        "\n",
        "# Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label_id\"])\n",
        "train_df, val_df  = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"label_id\"])\n",
        "\n",
        "# HF Dataset\n",
        "train_ds = Dataset.from_pandas(train_df[[\"text\",\"label_id\"]])\n",
        "val_ds   = Dataset.from_pandas(val_df[[\"text\",\"label_id\"]])\n",
        "test_ds  = Dataset.from_pandas(test_df[[\"text\",\"label_id\"]])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess(ex):\n",
        "    out = tokenizer(ex[\"text\"], truncation=True, max_length=256)\n",
        "    out[\"labels\"] = ex[\"label_id\"]\n",
        "    return out\n",
        "\n",
        "train_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
        "val_ds   = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
        "test_ds  = test_ds.map(preprocess, batched=True, remove_columns=test_ds.column_names)\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id\n",
        ")\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=8, lora_alpha=16, target_modules=[\"query\",\"value\",\"key\",\"dense\",\"fc1\",\"fc2\"],\n",
        "    lora_dropout=0.1, bias=\"none\", task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "model = get_peft_model(base_model, lora_cfg)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "metric_f1 = evaluate.load(\"f1\")\n",
        "metric_acc = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"f1\": f1, \"accuracy\": acc}\n",
        "\n",
        "args = TrainingArguments(\n",
        "    OUTPUT_DIR, per_device_train_batch_size=16, per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-4, num_train_epochs=3, weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\", save_strategy=\"epoch\", load_best_model_at_end=True,\n",
        "    logging_steps=50, fp16=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer, data_collator=collator, compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_res = trainer.evaluate(test_ds)\n",
        "print(\"Test metrics:\", eval_res)\n",
        "\n",
        "# Confusion matrix\n",
        "preds = np.argmax(trainer.predict(test_ds).predictions, axis=-1)\n",
        "cm = confusion_matrix(test_ds[\"labels\"], preds, labels=list(range(NUM_LABELS)))\n",
        "plt.figure()\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.title('Confusion Matrix'); plt.colorbar()\n",
        "plt.tight_layout(); plt.ylabel('True'); plt.xlabel('Pred')\n",
        "cm_path = os.path.join(OUTPUT_DIR, \"confusion_matrix.png\")\n",
        "plt.savefig(cm_path, dpi=150)\n",
        "\n",
        "# Save final metrics as a small text report\n",
        "with open(os.path.join(OUTPUT_DIR, \"report.txt\"), \"w\") as f:\n",
        "    for k,v in eval_res.items():\n",
        "        f.write(f\"{k}: {v}\\n\")\n",
        "print(\"Saved report and confusion matrix to\", OUTPUT_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}