{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QLoRA fineâ€‘tuning for distilbert-base-uncased\n",
        "\n",
        "Upload `dataset.csv` with `text,label` columns and set `DATASET_PATH` as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# If on Colab:\n",
        "# !pip install -q transformers datasets peft accelerate evaluate scikit-learn matplotlib bitsandbytes --upgrade\n",
        "\n",
        "import os, numpy as np, pandas as pd, evaluate, matplotlib.pyplot as plt\n",
        "from datasets import Dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          DataCollatorWithPadding, TrainingArguments, Trainer)\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "DATASET_PATH = \"/content/dataset.csv\"  # CSV with columns: text,label\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "USE_QLORA = True  # True -> 4bit QLoRA via bitsandbytes\n",
        "OUTPUT_DIR = \"outputs_distilbert_qlora_adv\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "assert \"text\" in df.columns and \"label\" in df.columns\n",
        "labels = sorted(df[\"label\"].astype(str).unique().tolist())\n",
        "label2id = {l:i for i,l in enumerate(labels)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "df[\"label_id\"] = df[\"label\"].astype(str).map(label2id)\n",
        "\n",
        "# Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label_id\"])\n",
        "train_df, val_df  = train_test_split(train_df, test_size=0.2, random_state=42, stratify=train_df[\"label_id\"])\n",
        "\n",
        "# HF Dataset\n",
        "train_ds = Dataset.from_pandas(train_df[[\"text\",\"label_id\"]])\n",
        "val_ds   = Dataset.from_pandas(val_df[[\"text\",\"label_id\"]])\n",
        "test_ds  = Dataset.from_pandas(test_df[[\"text\",\"label_id\"]])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def preprocess(ex):\n",
        "    out = tokenizer(ex[\"text\"], truncation=True, max_length=256)\n",
        "    out[\"labels\"] = ex[\"label_id\"]\n",
        "    return out\n",
        "\n",
        "train_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
        "val_ds   = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
        "test_ds  = test_ds.map(preprocess, batched=True, remove_columns=test_ds.column_names)\n",
        "\n",
        "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "bnb_config = None\n",
        "if USE_QLORA:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
        "    )\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME, num_labels=len(labels), id2label=id2label, label2id=label2id,\n",
        "        quantization_config=bnb_config, device_map=\"auto\"\n",
        "    )\n",
        "    base_model = prepare_model_for_kbit_training(base_model, use_gradient_checkpointing=True)\n",
        "else:\n",
        "    base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        MODEL_NAME, num_labels=len(labels), id2label=id2label, label2id=label2id\n",
        "    )\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16, lora_alpha=32,\n",
        "    target_modules=[\"query\",\"key\",\"value\",\"dense\",\"fc1\",\"fc2\",\"classifier\",\"intermediate.dense\",\"output.dense\"],\n",
        "    lora_dropout=0.05, bias=\"none\", task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "model = get_peft_model(base_model, lora_cfg)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels_np = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    f1_w = f1_score(labels_np, preds, average=\"weighted\")\n",
        "    f1_m = f1_score(labels_np, preds, average=\"macro\")\n",
        "    acc = accuracy_score(labels_np, preds)\n",
        "    return {\"f1_weighted\": f1_w, \"f1_macro\": f1_m, \"accuracy\": acc}\n",
        "\n",
        "args = TrainingArguments(\n",
        "    OUTPUT_DIR, per_device_train_batch_size=16, per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-4, num_train_epochs=4, weight_decay=0.05,\n",
        "    evaluation_strategy=\"epoch\", save_strategy=\"epoch\", load_best_model_at_end=True,\n",
        "    logging_steps=50, fp16=True, gradient_accumulation_steps=2,\n",
        "    save_total_limit=2, report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
        "    tokenizer=tokenizer, data_collator=collator, compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "eval_res = trainer.evaluate(test_ds)\n",
        "print(\"Test metrics:\", eval_res)\n",
        "\n",
        "# Confusion matrix\n",
        "preds = np.argmax(trainer.predict(test_ds).predictions, axis=-1)\n",
        "cm = confusion_matrix(test_ds[\"labels\"], preds, labels=list(range(len(labels))))\n",
        "plt.figure(); plt.imshow(cm, interpolation='nearest'); plt.title('Confusion Matrix'); plt.colorbar()\n",
        "plt.tight_layout(); plt.ylabel('True'); plt.xlabel('Pred')\n",
        "cm_path = os.path.join(OUTPUT_DIR, \"confusion_matrix.png\"); plt.savefig(cm_path, dpi=150)\n",
        "\n",
        "# Save adapter\n",
        "trainer.model.save_pretrained(os.path.join(OUTPUT_DIR, \"lora_adapter\"))\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Save final metrics\n",
        "with open(os.path.join(OUTPUT_DIR, \"report.json\"), \"w\") as f:\n",
        "    import json; json.dump(eval_res, f, indent=2)\n",
        "print(\"Saved report, confusion matrix, and LoRA adapter to\", OUTPUT_DIR)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}