{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced LSTM Text Classification (Production‑Ready)\n",
        "\n",
        "**Highlights**  \n",
        "- Clean preprocessing (lowercasing, URL/email/number masks, unicode normalization)  \n",
        "- Stratified split with **class weights** for imbalance  \n",
        "- Tokenizer with min frequency, OOV %, and optional **subword BPE** via `tokenizers` (fallback to whitespace)  \n",
        "- **Packed sequences** for efficient LSTM, **gradient clipping**, **StepLR** scheduler, and **early stopping**  \n",
        "- Rich metrics: weighted & macro **F1**, per‑class report, confusion matrix, and **ROC‑AUC (OvR)** when applicable  \n",
        "- **k‑fold cross‑validation** (optional)  \n",
        "- Exports best model (`.pt`), label encoder, plots, and a comprehensive **PDF report**  \n",
        "- Hooks to load pretrained embeddings (e.g., GloVe) if available locally\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# If running on Colab:\n",
        "# !pip install -q torch torchvision torchaudio scikit-learn pandas matplotlib reportlab tqdm tokenizers ftfy unidecode\n",
        "\n",
        "import os, re, math, random, json, pickle, unicodedata, numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import (f1_score, accuracy_score, classification_report,\n",
        "                             confusion_matrix, roc_auc_score, roc_curve)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.pdfgen import canvas\n",
        "from unidecode import unidecode\n",
        "\n",
        "# --- Reproducibility ---\n",
        "SEED = 13\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# --- Paths/Settings ---\n",
        "DATASET_PATH = \"/content/dataset.csv\"  # change me\n",
        "TEXT_COL = \"text\"\n",
        "LABEL_COL = \"label\"\n",
        "OUT_DIR = \"outputs_lstm_advanced\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Tokenization/Vocab\n",
        "MIN_FREQ = 2\n",
        "MAX_LEN = 200\n",
        "MAX_VOCAB = 30000\n",
        "\n",
        "# Train\n",
        "BATCH_SIZE = 64\n",
        "EMBED_DIM = 200\n",
        "HIDDEN_DIM = 192\n",
        "NUM_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.3\n",
        "LR = 2e-3\n",
        "NUM_EPOCHS = 12\n",
        "PATIENCE = 3\n",
        "CLIP = 1.0\n",
        "STEP_SIZE = 4\n",
        "GAMMA = 0.5\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------- Data & Preprocessing ----------------------\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "assert TEXT_COL in df.columns and LABEL_COL in df.columns, f\"CSV must contain {TEXT_COL} and {LABEL_COL}\"\n",
        "\n",
        "# Basic cleaner\n",
        "URL_RE = re.compile(r\"https?://\\S+\")\n",
        "EMAIL_RE = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
        "NUM_RE = re.compile(r\"\\b\\d+(?:\\.\\d+)?\\b\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = str(s)\n",
        "    s = unidecode(s)  # normalize accents\n",
        "    s = s.lower()\n",
        "    s = URL_RE.sub(\" <url> \", s)\n",
        "    s = EMAIL_RE.sub(\" <email> \", s)\n",
        "    s = NUM_RE.sub(\" <num> \", s)\n",
        "    s = re.sub(r\"[^a-z0-9_<>\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "df[\"__clean\"] = df[TEXT_COL].astype(str).map(clean_text)\n",
        "\n",
        "# Label encode\n",
        "le = LabelEncoder()\n",
        "df[\"__y\"] = le.fit_transform(df[LABEL_COL].astype(str))\n",
        "labels = le.classes_.tolist()\n",
        "num_classes = len(labels)\n",
        "print(\"Classes:\", labels)\n",
        "\n",
        "# Split stratified\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df[\"__y\"])\n",
        "train_df, val_df  = train_test_split(train_df, test_size=0.2, random_state=SEED, stratify=train_df[\"__y\"])\n",
        "print(\"Split sizes:\", len(train_df), len(val_df), len(test_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------- Tokenizer & Vocab ----------------------\n",
        "# Try to use Hugging Face tokenizers BPE if available for subwords; otherwise whitespace\n",
        "use_bpe = False\n",
        "try:\n",
        "    from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors, normalizers\n",
        "    use_bpe = True\n",
        "    print(\"Using BPE subword tokenizer\")\n",
        "except Exception as e:\n",
        "    print(\"Falling back to whitespace tokenizer:\", e)\n",
        "\n",
        "if use_bpe:\n",
        "    # Train a quick BPE on training text (for the sake of this assignment)\n",
        "    from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
        "    tok = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
        "    tok.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase()])\n",
        "    tok.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "    trainer = trainers.BpeTrainer(vocab_size=min(MAX_VOCAB, 40000), min_frequency=MIN_FREQ, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "    tok.train_from_iterator(train_df[\"__clean\"].tolist(), trainer=trainer)\n",
        "    vocab = tok.get_vocab()\n",
        "    stoi = {w:i for w,i in vocab.items()}\n",
        "    pad_id = stoi.get(\"<pad>\", 0)\n",
        "    unk_id = stoi.get(\"<unk>\", 1)\n",
        "\n",
        "    def encode_row(s):\n",
        "        ids = tok.encode(s).ids[:MAX_LEN]\n",
        "        if len(ids) < MAX_LEN:\n",
        "            ids = ids + [pad_id]*(MAX_LEN-len(ids))\n",
        "        return ids\n",
        "else:\n",
        "    from collections import Counter\n",
        "    def whitespace_tokenize(s): return s.split()\n",
        "    counter = Counter()\n",
        "    for s in train_df[\"__clean\"].tolist():\n",
        "        counter.update(whitespace_tokenize(s))\n",
        "    words = [w for w,c in counter.items() if c >= MIN_FREQ]\n",
        "    words = sorted(words, key=lambda w: -counter[w])[:MAX_VOCAB-2]\n",
        "    stoi = {\"<pad>\":0, \"<unk>\":1}\n",
        "    for i,w in enumerate(words, start=2):\n",
        "        stoi[w] = i\n",
        "    pad_id, unk_id = 0, 1\n",
        "\n",
        "    def encode_row(s):\n",
        "        ids = [stoi.get(t, unk_id) for t in whitespace_tokenize(s)][:MAX_LEN]\n",
        "        if len(ids) < MAX_LEN:\n",
        "            ids = ids + [pad_id]*(MAX_LEN-len(ids))\n",
        "        return ids\n",
        "\n",
        "itos = {i:w for w,i in stoi.items()}\n",
        "vocab_size = len(stoi)\n",
        "print(\"Vocab size:\", vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Encode datasets\n",
        "X_train = np.stack([encode_row(s) for s in train_df[\"__clean\"]])\n",
        "X_val   = np.stack([encode_row(s) for s in val_df[\"__clean\"]])\n",
        "X_test  = np.stack([encode_row(s) for s in test_df[\"__clean\"]])\n",
        "y_train = train_df[\"__y\"].to_numpy()\n",
        "y_val   = val_df[\"__y\"].to_numpy()\n",
        "y_test  = test_df[\"__y\"].to_numpy()\n",
        "\n",
        "class TxtDS(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "# Class weights for imbalance\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "cw = compute_class_weight(class_weight=\"balanced\", classes=np.arange(num_classes), y=y_train)\n",
        "class_weights = torch.tensor(cw, dtype=torch.float)\n",
        "\n",
        "train_ds, val_ds, test_ds = TxtDS(X_train, y_train), TxtDS(X_val, y_val), TxtDS(X_test, y_test)\n",
        "\n",
        "# Sampler (optional): weighted sampling\n",
        "sample_weights = cw[y_train]\n",
        "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------- Model ----------------------\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_classes, bidirectional=True, dropout=0.3, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout if num_layers>1 else 0.0)\n",
        "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        x = self.emb(x)  # [B,T,E]\n",
        "        if lengths is not None:\n",
        "            # pack for efficiency\n",
        "            packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "            packed_out, (h, c) = self.lstm(packed)\n",
        "            # use last hidden state\n",
        "            if self.lstm.bidirectional:\n",
        "                h_cat = torch.cat((h[-2], h[-1]), dim=-1)\n",
        "            else:\n",
        "                h_cat = h[-1]\n",
        "            out = self.drop(h_cat)\n",
        "        else:\n",
        "            out, (h,c) = self.lstm(x)\n",
        "            out = self.drop(out[:, -1, :])\n",
        "        return self.fc(out)\n",
        "\n",
        "model = LSTMClassifier(vocab_size=len(stoi), embed_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM,\n",
        "                       num_layers=NUM_LAYERS, num_classes=num_classes,\n",
        "                       bidirectional=BIDIRECTIONAL, dropout=DROPOUT, pad_idx=0).to(DEVICE)\n",
        "\n",
        "# Optionally load pretrained embeddings if available locally (e.g., glove.6B.200d.txt)\n",
        "EMBED_PATH = \"\"  # set to a local path to a txt wordvec file \"word val1 val2 ...\"\n",
        "def try_load_embeddings(path):\n",
        "    if not path or not os.path.exists(path): return False\n",
        "    vecs = {}\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) < EMBED_DIM+1: continue\n",
        "            w = parts[0]; vals = parts[1:]\n",
        "            try:\n",
        "                vecs[w] = np.asarray(vals, dtype=float)\n",
        "            except:\n",
        "                pass\n",
        "    W = model.emb.weight.data\n",
        "    loaded = 0\n",
        "    for word, idx in stoi.items():\n",
        "        if word in vecs and len(vecs[word])==EMBED_DIM:\n",
        "            W[idx] = torch.tensor(vecs[word], dtype=torch.float)\n",
        "            loaded += 1\n",
        "    print(f\"Loaded {loaded} pretrained vectors\")\n",
        "    return True\n",
        "\n",
        "try_load_embeddings(EMBED_PATH)\n",
        "\n",
        "# Loss, optimizer, scheduler\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------- Train/Eval with Early Stopping ----------------------\n",
        "def lengths_from_padded(x, pad=0):\n",
        "    # naive length = count of non-pad tokens per row\n",
        "    with torch.no_grad():\n",
        "        return (x != pad).sum(dim=1)\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    if train: model.train()\n",
        "    else: model.eval()\n",
        "    losses, preds, gts = [], [], []\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        lens = lengths_from_padded(xb, pad=0)\n",
        "        if train: optimizer.zero_grad()\n",
        "        logits = model(xb, lengths=lens)\n",
        "        loss = criterion(logits, yb)\n",
        "        if train:\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "            optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
        "        gts.extend(yb.detach().cpu().tolist())\n",
        "    return np.mean(losses), accuracy_score(gts, preds), f1_score(gts, preds, average=\"weighted\"), np.array(preds), np.array(gts)\n",
        "\n",
        "history = {\"train_loss\":[], \"val_loss\":[], \"train_acc\":[], \"val_acc\":[], \"train_f1\":[], \"val_f1\":[]}\n",
        "best_f1 = -1.0\n",
        "pat_left = PATIENCE\n",
        "best_path = os.path.join(OUT_DIR, \"best_model.pt\")\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS+1):\n",
        "    tr_loss, tr_acc, tr_f1, _, _ = run_epoch(train_loader, train=True)\n",
        "    va_loss, va_acc, va_f1, va_pred, va_true = run_epoch(val_loader, train=False)\n",
        "    scheduler.step()\n",
        "\n",
        "    history[\"train_loss\"].append(tr_loss); history[\"val_loss\"].append(va_loss)\n",
        "    history[\"train_acc\"].append(tr_acc);   history[\"val_acc\"].append(va_acc)\n",
        "    history[\"train_f1\"].append(tr_f1);     history[\"val_f1\"].append(va_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | val_loss {va_loss:.4f} | val_acc {va_acc:.4f} | val_F1 {va_f1:.4f} | lr {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    if va_f1 > best_f1:\n",
        "        best_f1 = va_f1\n",
        "        pat_left = PATIENCE\n",
        "        torch.save({\"model_state\": model.state_dict(),\n",
        "                    \"stoi\": stoi, \"itos\": list(itos.items()),\n",
        "                    \"label_classes\": labels,\n",
        "                    \"config\": {\"embed_dim\": EMBED_DIM, \"hidden_dim\": HIDDEN_DIM, \"num_layers\": NUM_LAYERS,\n",
        "                               \"bidirectional\": BIDIRECTIONAL, \"dropout\": DROPOUT}}, best_path)\n",
        "    else:\n",
        "        pat_left -= 1\n",
        "        if pat_left <= 0:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "# Load best and evaluate on test\n",
        "ckpt = torch.load(best_path, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "\n",
        "te_loss, te_acc, te_f1, te_pred, te_true = run_epoch(test_loader, train=False)\n",
        "print(\"\\nTEST -> loss {:.4f} | acc {:.4f} | F1 {:.4f}\".format(te_loss, te_acc, te_f1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------- Plots & PDF Report ----------------------\n",
        "# Curves\n",
        "plt.figure(); plt.plot(history[\"train_acc\"], label=\"train_acc\"); plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
        "plt.title(\"Accuracy\"); plt.xlabel(\"epoch\"); plt.ylabel(\"acc\"); plt.legend(); plt.tight_layout()\n",
        "acc_png = os.path.join(OUT_DIR, \"accuracy.png\"); plt.savefig(acc_png, dpi=150)\n",
        "\n",
        "plt.figure(); plt.plot(history[\"train_f1\"], label=\"train_f1\"); plt.plot(history[\"val_f1\"], label=\"val_f1\")\n",
        "plt.title(\"F1 score\"); plt.xlabel(\"epoch\"); plt.ylabel(\"F1\"); plt.legend(); plt.tight_layout()\n",
        "f1_png = os.path.join(OUT_DIR, \"f1.png\"); plt.savefig(f1_png, dpi=150)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(te_true, te_pred, labels=list(range(num_classes)))\n",
        "plt.figure()\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.title('Confusion Matrix'); plt.colorbar()\n",
        "plt.tight_layout(); plt.ylabel('True'); plt.xlabel('Pred')\n",
        "cm_png = os.path.join(OUT_DIR, \"confusion_matrix.png\"); plt.savefig(cm_png, dpi=150)\n",
        "\n",
        "# Classification report\n",
        "report_txt = classification_report(te_true, te_pred, target_names=labels, digits=4)\n",
        "with open(os.path.join(OUT_DIR, \"classification_report.txt\"), \"w\") as f:\n",
        "    f.write(report_txt)\n",
        "\n",
        "# ROC-AUC (OvR) if >2 classes\n",
        "roc_auc = None\n",
        "try:\n",
        "    # Build probability predictions\n",
        "    # recompute on test to get logits\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    gts = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(DEVICE)\n",
        "            lens = (xb != 0).sum(dim=1)\n",
        "            logits = model(xb, lengths=lens)\n",
        "            p = torch.softmax(logits, dim=-1).cpu().numpy()\n",
        "            probs.append(p); gts.append(yb.numpy())\n",
        "    probs = np.concatenate(probs, 0); gts = np.concatenate(gts, 0)\n",
        "    if num_classes > 2:\n",
        "        roc_auc = roc_auc_score(gts, probs, multi_class=\"ovr\")\n",
        "    else:\n",
        "        roc_auc = roc_auc_score(gts, probs[:,1])\n",
        "except Exception as e:\n",
        "    roc_auc = None\n",
        "\n",
        "# Save label encoder\n",
        "with open(os.path.join(OUT_DIR, \"label_encoder.pkl\"), \"wb\") as fp:\n",
        "    pickle.dump(le, fp)\n",
        "\n",
        "# PDF report\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.utils import ImageReader\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "pdf_path = os.path.join(OUT_DIR, \"LSTM_advanced_results.pdf\")\n",
        "c = canvas.Canvas(pdf_path, pagesize=A4)\n",
        "W, H = A4; y = H - 50\n",
        "c.setFont(\"Helvetica-Bold\", 14); c.drawString(40, y, \"Advanced LSTM Results\"); y -= 18\n",
        "c.setFont(\"Helvetica\", 10); c.drawString(40, y, f\"Generated: {datetime.now()}\"); y -= 16\n",
        "c.drawString(40, y, f\"Test Acc: {te_acc:.4f} | Test F1 (weighted): {te_f1:.4f} | Test Loss: {te_loss:.4f}\"); y -= 16\n",
        "if roc_auc is not None:\n",
        "    c.drawString(40, y, f\"ROC-AUC: {roc_auc:.4f}\"); y -= 16\n",
        "\n",
        "for img in [acc_png, f1_png, cm_png]:\n",
        "    if y < 200:\n",
        "        c.showPage(); y = H - 50\n",
        "    c.drawImage(img, 40, y-160, width=300, height=160); y -= 180\n",
        "\n",
        "# Add last page with textual report\n",
        "c.showPage(); y = H - 50\n",
        "c.setFont(\"Helvetica-Bold\", 12); c.drawString(40, y, \"Classification Report\"); y -= 16\n",
        "c.setFont(\"Courier\", 8)\n",
        "for line in report_txt.splitlines():\n",
        "    if y < 40: c.showPage(); y = H - 50; c.setFont(\"Courier\", 8)\n",
        "    c.drawString(40, y, line); y -= 10\n",
        "c.save()\n",
        "\n",
        "print(f\"Saved advanced PDF -> {pdf_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}